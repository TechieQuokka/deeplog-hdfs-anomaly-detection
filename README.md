# DeepLog HDFS Anomaly Detection - Reproduction Study

Reproduction of the DeepLog paper's HDFS log anomaly detection experiment.

**Paper**: Min Du, Feifei Li, Guineng Zheng, Vivek Srikumar. "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning." CCS 2017.

## ğŸ“Š Results

### Paper vs Reproduction (g=9)

| Metric | Paper | Ours | Gap |
|--------|-------|------|-----|
| False Positives | 833 | 6,417 | +5,584 |
| False Negatives | 619 | 1,326 | +707 |
| Precision | 95.1% | 59.2% | -35.9pp |
| Recall | 96.4% | 87.5% | -8.9pp |
| **F1-Score** | **96.0%** | **70.7%** | **-25.3pp** |

### Top-g Sensitivity (best result at g=13)

| Metric | g=9 | g=13 (best F1) |
|--------|-----|-----------------|
| FP | 6,417 | 2,252 |
| FN | 1,326 | 2,307 |
| Precision | 59.2% | 78.7% |
| Recall | 87.5% | 78.3% |
| F1 | 70.7% | 78.5% |

### Data Split Verification

| Split | Paper | Ours |
|-------|-------|------|
| Train (Normal) | 4,855 | 4,855 |
| Test (Normal) | 553,366 | 553,368 |
| Test (Anomaly) | 16,838 | 10,647 |

### Gap Analysis

- **FPê°€ 7.7ë°° ë†’ìŒ**: session-level OR aggregationì´ sequence-level FPë¥¼ ì¦í­ì‹œí‚´
- **FP ì„¸ì…˜ íŠ¹ì„±**: ëŒ€ë¶€ë¶„ì˜ FP ì„¸ì…˜ì€ ì†Œìˆ˜ì˜ ì‹œí€€ìŠ¤ë§Œ anomalyë¡œ íŒì •ë¨
- **Rank ë¶„í¬**: ì •ìƒ ì‹œí€€ìŠ¤ì˜ ëŒ€ë‹¤ìˆ˜ê°€ rank 0~1ì— ìœ„ì¹˜í•˜ë‚˜, ê¸´ ê¼¬ë¦¬ ë¶„í¬ê°€ FPë¥¼ ìœ ë°œ
- **ìµœì  g=13**: gë¥¼ ë†’ì´ë©´ FP ê°ì†Œ/FN ì¦ê°€ trade-off, F1 ìµœëŒ€ 78.5%

> ìƒì„¸ ì‹œê°í™” ë¶„ì„ì€ `notebooks/analysis.ipynb` ì°¸ì¡°.

## ğŸ—ï¸ Project Structure

```
deeplog-hdfs-reproduction/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # Configuration and hyperparameters
â”‚   â”œâ”€â”€ preprocessing.py    # Data preprocessing pipeline
â”‚   â”œâ”€â”€ dataset.py          # PyTorch Dataset and DataLoader
â”‚   â”œâ”€â”€ model.py            # DeepLog LSTM model
â”‚   â”œâ”€â”€ train.py            # Training pipeline
â”‚   â”œâ”€â”€ evaluate.py         # Evaluation with top-g prediction
â”‚   â””â”€â”€ utils.py            # Utility functions
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analysis.ipynb      # Results visualization and analysis
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/          # Preprocessed sequences (generated)
â”œâ”€â”€ checkpoints/            # Model checkpoints (generated)
â”œâ”€â”€ results/                # Evaluation results (generated)
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture.md     # System architecture documentation
â”œâ”€â”€ main.py                 # Main execution pipeline
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Installation

```bash
python3.11 -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

### 2. Data Setup

Required files (HDFS_v1 from [LogHub](https://github.com/logpai/loghub)):
- `Event_traces.csv` - Log event sequences by block_id
- `anomaly_label.csv` - Ground truth labels

Update `DATA_ROOT` in `src/config.py` to point to your data directory.

### 3. Run Complete Pipeline

```bash
# Run all stages: preprocessing â†’ training â†’ evaluation
python main.py --all
```

### 4. Run Individual Stages

```bash
python main.py --preprocess   # Data preprocessing
python main.py --train        # Model training
python main.py --evaluate     # Model evaluation
```

## ğŸ”§ Configuration

All hyperparameters are centralized in `src/config.py`:

### Model Hyperparameters (Paper Specification)
```python
NUM_CLASSES = 29        # n: Number of distinct log keys
WINDOW_SIZE = 10        # h: History window size
NUM_LAYERS = 2          # L: Number of LSTM layers
HIDDEN_SIZE = 64        # Î±: LSTM hidden units
TOP_G = 9               # g: Top-g prediction threshold
```

### Training Configuration
```python
BATCH_SIZE = 128
LEARNING_RATE = 0.001
NUM_EPOCHS = 100
EARLY_STOPPING_PATIENCE = 10
EMBEDDING_DIM = 64
```

## ğŸ”¬ Reproduction Notes

### Paper Specifications Implemented
- Window size h=10
- LSTM layers L=2, hidden units Î±=64
- Number of log keys n=29
- Top-g prediction with g=9
- Session-level anomaly aggregation (OR)

### Data Split Fix (Critical)

ë…¼ë¬¸ì—ì„œ "first 100,000 log entries"ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ì´ ì¬í˜„ì—ì„œ ë°œê²¬í•œ í•µì‹¬ ì‚¬í•­:

- `Event_traces.csv`ì˜ í–‰ ìˆœì„œëŠ” `HDFS.log`ì—ì„œì˜ ìµœì´ˆ ë“±ì¥ ìˆœì„œì™€ ì¼ì¹˜ (100/100 ê²€ì¦)
- ì˜¬ë°”ë¥¸ ë¶„í• : ëˆ„ì  **ì •ìƒ ì„¸ì…˜ ìˆ˜**ê°€ 4,855ì— ë„ë‹¬í•  ë•Œê¹Œì§€ì˜ í–‰ì„ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©
- ì˜ëª»ëœ ë¶„í• : ëˆ„ì  **ì´ë²¤íŠ¸ ìˆ˜**ë¡œ 100,000ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ë©´ 3,684 ì„¸ì…˜ (ë¶€ì¡±)

### Remaining Performance Gap

ë…¼ë¬¸ ëŒ€ë¹„ FPê°€ í¬ê²Œ ë†’ì€ ì›ì¸ í›„ë³´:
1. Session-level OR aggregationì´ sequence-level FPë¥¼ ì¦í­
2. ë…¼ë¬¸ì— ê¸°ìˆ ë˜ì§€ ì•Šì€ ì¶”ê°€ ê¸°ë²•ì´ ìˆì„ ê°€ëŠ¥ì„± (ì˜ˆ: threshold tuning, post-processing)
3. ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë˜ëŠ” í•™ìŠµ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ ì°¨ì´

### Training Details
- Best model epoch: 33 (val loss: 0.1799)
- Training sequences: 78,093
- Test sequences: 5,389,938
- Trainable parameters: 70,301

## ğŸ“ Citation

```bibtex
@inproceedings{du2017deeplog,
  title={DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning},
  author={Du, Min and Li, Feifei and Zheng, Guineng and Srikumar, Vivek},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1285--1298},
  year={2017}
}
```

## ğŸ“„ License

This is a reproduction study for research and educational purposes.

---

**Last Updated**: 2026-02-07
